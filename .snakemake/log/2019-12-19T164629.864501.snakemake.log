Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	fastQC
	2

[Thu Dec 19 16:46:31 2019]
rule fastQC:
    input: s3Combine/1.r1.fastq, s3Combine/2.r1.fastq, s3Combine/3.r1.fastq, s3Combine/4.r1.fastq, s3Combine/5.r1.fastq, s3Combine/6.r1.fastq, s3Combine/1.r2.fastq, s3Combine/2.r2.fastq, s3Combine/3.r2.fastq, s3Combine/4.r2.fastq, s3Combine/5.r2.fastq, s3Combine/6.r2.fastq
    output: s4FastQC/r1.fastq.gz, s4FastQC/r2.fastq.gz, s4FastQC/r1_fastqc.html, s4FastQC/r2_fastqc.html, s4FastQC/r1_fastqc.zip, s4FastQC/r2_fastqc.zip
    jobid: 1

[Thu Dec 19 16:46:36 2019]
Finished job 1.
1 of 2 steps (50%) done

[Thu Dec 19 16:46:36 2019]
localrule all:
    input: s4FastQC/r1.fastq.gz, s4FastQC/r2.fastq.gz, s4FastQC/r1_fastqc.html, s4FastQC/r2_fastqc.html, s4FastQC/r1_fastqc.zip, s4FastQC/r2_fastqc.zip
    jobid: 0

[Thu Dec 19 16:46:36 2019]
Finished job 0.
2 of 2 steps (100%) done
Complete log: /home/xinyut/workflows/16s-pipeline/.snakemake/log/2019-12-19T164629.864501.snakemake.log
