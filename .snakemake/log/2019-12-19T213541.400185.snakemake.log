Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	DADA2
	1	all
	2

[Thu Dec 19 21:35:42 2019]
rule DADA2:
    input: s3Combine
    output: s5DADA2/DADA2_raw.rda
    jobid: 1

[Thu Dec 19 21:35:54 2019]
Error in rule DADA2:
    jobid: 1
    output: s5DADA2/DADA2_raw.rda

RuleException:
CalledProcessError in line 152 of /home/xinyut/workflows/16s-pipeline/microbiome.smk:
Command 'set -x; Rscript --vanilla /home/xinyut/workflows/16s-pipeline/.snakemake/scripts/tmp4um5j1xx.dada2_copy.R' returned non-zero exit status 1.
  File "/home/xinyut/workflows/16s-pipeline/microbiome.smk", line 152, in __rule_DADA2
  File "/home/xinyut/miniconda3/lib/python3.7/concurrent/futures/thread.py", line 57, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/xinyut/workflows/16s-pipeline/.snakemake/log/2019-12-19T213541.400185.snakemake.log
