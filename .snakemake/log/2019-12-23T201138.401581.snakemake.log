The flag 'directory' used in rule all is only valid for outputs, not inputs.
The flag 'directory' used in rule all is only valid for outputs, not inputs.
Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	fastqcScore
	2

[Mon Dec 23 20:11:39 2019]
rule fastqcScore:
    input: s4FastQC/r1_fastqc.zip, s4FastQC/r2_fastqc.zip
    output: s4FastQC/r1_fastqc, s4FastQC/r2_fastqc, tbl1, tbl2
    jobid: 1

[Mon Dec 23 20:11:40 2019]
Error in rule fastqcScore:
    jobid: 1
    output: s4FastQC/r1_fastqc, s4FastQC/r2_fastqc, tbl1, tbl2

RuleException:
CalledProcessError in line 148 of /home/xinyut/workflows/16s-pipeline/Snakefile:
Command 'set -x; /home/xinyut/miniconda3/envs/16s-pipeline/bin/python3.7 /home/xinyut/workflows/16s-pipeline/.snakemake/scripts/tmpwp4las5y.fastqcscore.py' returned non-zero exit status 1.
  File "/home/xinyut/workflows/16s-pipeline/Snakefile", line 148, in __rule_fastqcScore
  File "/home/xinyut/miniconda3/envs/16s-pipeline/lib/python3.7/concurrent/futures/thread.py", line 57, in run
Removing output files of failed job fastqcScore since they might be corrupted:
s4FastQC/r1_fastqc, s4FastQC/r2_fastqc
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/xinyut/workflows/16s-pipeline/.snakemake/log/2019-12-23T201138.401581.snakemake.log
