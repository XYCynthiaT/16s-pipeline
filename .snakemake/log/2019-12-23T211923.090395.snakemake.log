The flag 'directory' used in rule all is only valid for outputs, not inputs.
The flag 'directory' used in rule all is only valid for outputs, not inputs.
Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	fastqcScore
	2

[Mon Dec 23 21:19:24 2019]
rule fastqcScore:
    input: s4FastQC/r1_fastqc.zip, s4FastQC/r2_fastqc.zip
    output: s4FastQC/r1_fastqc, s4FastQC/r2_fastqc, cutLen.csv
    jobid: 1

[Mon Dec 23 21:19:25 2019]
Error in rule fastqcScore:
    jobid: 1
    output: s4FastQC/r1_fastqc, s4FastQC/r2_fastqc, cutLen.csv

RuleException:
CalledProcessError in line 146 of /home/xinyut/workflows/16s-pipeline/Snakefile:
Command 'set -x; /home/xinyut/miniconda3/envs/16s-pipeline/bin/python3.7 /home/xinyut/workflows/16s-pipeline/.snakemake/scripts/tmpq12lk2ae.fastqcscore.py' returned non-zero exit status 1.
  File "/home/xinyut/workflows/16s-pipeline/Snakefile", line 146, in __rule_fastqcScore
  File "/home/xinyut/miniconda3/envs/16s-pipeline/lib/python3.7/concurrent/futures/thread.py", line 57, in run
Removing output files of failed job fastqcScore since they might be corrupted:
s4FastQC/r1_fastqc, s4FastQC/r2_fastqc
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/xinyut/workflows/16s-pipeline/.snakemake/log/2019-12-23T211923.090395.snakemake.log
