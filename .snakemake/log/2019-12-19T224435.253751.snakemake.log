Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	DADA2
	1	all
	2

[Thu Dec 19 22:44:36 2019]
rule DADA2:
    input: s3Combine
    output: s5DADA2/DADA2_raw.rda, s5Dada2/img/ploterrF.png, s5Dada2/img/ploterrR.png
    jobid: 1

Will exit after finishing currently running jobs.
[Thu Dec 19 22:54:42 2019]
Error in rule DADA2:
    jobid: 1
    output: s5DADA2/DADA2_raw.rda, s5Dada2/img/ploterrF.png, s5Dada2/img/ploterrR.png

RuleException:
CalledProcessError in line 159 of /home/xinyut/workflows/16s-pipeline/microbiome.smk:
Command 'set -x; Rscript --vanilla /home/xinyut/workflows/16s-pipeline/.snakemake/scripts/tmpnl6p7912.dada2_copy.R' died with <Signals.SIGTERM: 15>.
  File "/home/xinyut/workflows/16s-pipeline/microbiome.smk", line 159, in __rule_DADA2
  File "/home/xinyut/miniconda3/envs/16s-pipeline/lib/python3.7/concurrent/futures/thread.py", line 57, in run
Will exit after finishing currently running jobs.
Shutting down, this might take some time.
Complete log: /home/xinyut/workflows/16s-pipeline/.snakemake/log/2019-12-19T224435.253751.snakemake.log
