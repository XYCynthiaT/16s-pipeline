Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 50
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	DADA2
	1	all
	1	taxaCleaning
	1	taxonomy
	4

[Fri Dec 20 14:23:16 2019]
rule DADA2:
    input: s3Combine
    output: s5DADA2/DADA2_seqtab_nochim.rda, s5DADA2/img/ploterrF.png, s5DADA2/img/ploterrR.png
    jobid: 3

Terminating processes on user request, this might take some time.
[Fri Dec 20 14:23:25 2019]
Error in rule DADA2:
    jobid: 3
    output: s5DADA2/DADA2_seqtab_nochim.rda, s5DADA2/img/ploterrF.png, s5DADA2/img/ploterrR.png

RuleException:
CalledProcessError in line 154 of /home/xinyut/workflows/16s-pipeline/microbiome.smk:
Command 'set -x; Rscript --vanilla /home/xinyut/workflows/16s-pipeline/.snakemake/scripts/tmpeu_qcubt.dada2.R' returned non-zero exit status 1.
  File "/home/xinyut/workflows/16s-pipeline/microbiome.smk", line 154, in __rule_DADA2
  File "/home/xinyut/miniconda3/envs/16s-pipeline/lib/python3.7/concurrent/futures/thread.py", line 57, in run
Complete log: /home/xinyut/workflows/16s-pipeline/.snakemake/log/2019-12-20T142315.176987.snakemake.log
